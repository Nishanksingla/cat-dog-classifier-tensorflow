{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "# from cnn_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.nn.softmax_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 227\n",
    "data_file = \"dataset.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(input_data,channels_in, filter_size,num_filters,stride_size,pad=\"SAME\",name=\"conv\",weight_name=\"W1\"):\n",
    "    with tf.name_scope(name):\n",
    "        W=tf.get_variable(weight_name,shape=[filter_size,filter_size,channels_in,num_filters],initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "        conv = tf.nn.conv2d(input_data,W,strides=[1,stride_size,stride_size,1],padding=pad)\n",
    "        act = tf.nn.relu(conv)\n",
    "        tf.summary.histogram(\"weights\", W)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def alexnet_model(input_data,image_channels):\n",
    "    #input_size = [227*227*3]\n",
    "    conv1 = conv_layer(input_data,image_channels,11,96,4,\"VALID\",\"conv1\",\"W1\")\n",
    "    #output size = [55*55*96]\n",
    "    P1 = tf.nn.max_pool(conv1,ksize=[1,3,3,1], strides=[1,2,2,1],padding=\"VALID\")\n",
    "    #output_size = [27*27*96]\n",
    "    \n",
    "    conv2 = conv_layer(P1,96,5,256,1,\"SAME\",\"conv2\",\"W2\")\n",
    "    #output_size = [27*27*256]\n",
    "    P2 = tf.nn.max_pool(conv2,ksize=[1,3,3,1], strides=[1,2,2,1],padding=\"VALID\")\n",
    "    #output_size = [13*13*256]\n",
    "    \n",
    "    conv3 = conv_layer(P2,256,3,384,1,\"SAME\",\"conv3\",\"W3\")\n",
    "    #output_size = [13*13*384]\n",
    "    \n",
    "    conv4 = conv_layer(conv3,384,3,384,1,\"SAME\",\"conv4\",\"W4\")\n",
    "    #output_size = [13*13*384]\n",
    "    \n",
    "    conv5 = conv_layer(conv4,384,3,256,1,\"SAME\",\"conv5\",\"W5\")\n",
    "    #output_size = [13*13*256]\n",
    "    \n",
    "    P3 = tf.nn.max_pool(conv5,ksize=[1,3,3,1], strides=[1,2,2,1],padding=\"VALID\")\n",
    "    #output_size = [6*6*256]\n",
    "    \n",
    "    P3 = tf.contrib.layers.flatten(P3)\n",
    "    \n",
    "    fc6 = tf.contrib.layers.fully_connected(P3,4096,scope=\"fc6\")\n",
    "    tf.summary.histogram(\"fc6/relu\", fc6)\n",
    "    \n",
    "    dropout1 = tf.layers.dropout(inputs=fc6, rate=0.5)\n",
    "    \n",
    "    fc7 = tf.contrib.layers.fully_connected(dropout1,4096,scope=\"fc7\")\n",
    "    tf.summary.histogram(\"fc7/relu\", fc7)\n",
    "    \n",
    "    dropout2 = tf.layers.dropout(inputs=fc7, rate=0.5)\n",
    "    \n",
    "    fc8 = tf.contrib.layers.fully_connected(dropout2,2,activation_fn=None, scope=\"fc8\")\n",
    "    tf.summary.histogram(\"fc8\", fc8)\n",
    "    \n",
    "    return fc8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    newData=[]\n",
    "    labels=[]\n",
    "    print(\"preparing data....\")\n",
    "    for sample in data:\n",
    "        img_path,label = sample.strip().split(\" \")\n",
    "        \n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        img[:, :, 0] = cv2.equalizeHist(img[:, :, 0])\n",
    "        img[:, :, 1] = cv2.equalizeHist(img[:, :, 1])\n",
    "        img[:, :, 2] = cv2.equalizeHist(img[:, :, 2])\n",
    "\n",
    "        #Image Resizing\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_CUBIC)\n",
    "        \n",
    "        #img = Image.open(img_path)\n",
    "        #img = img.resize((IMG_SIZE,IMG_SIZE))\n",
    "        img = np.array(img)\n",
    "        img = img/255\n",
    "        newData.append(img)\n",
    "        if \"cat\" in img_path:\n",
    "            labels.append(np.array([1,0]))\n",
    "        elif \"dog\" in img_path:\n",
    "            labels.append(np.array([0,1]))\n",
    "    print(\"preparing data completed.\")\n",
    "    return np.array(newData),np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(data_file,\"r\")\n",
    "data = f.readlines()\n",
    "random.shuffle(data)\n",
    "print(len(data))\n",
    "train_data = data[:20000]\n",
    "val_data = data[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "learning_rate = 0.001\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    X = tf.placeholder(tf.float32, shape=(None,IMG_SIZE,IMG_SIZE , 3),name=\"X\")\n",
    "    tf.summary.image('input', X, BATCH_SIZE)\n",
    "    Y = tf.placeholder(tf.float32,shape=(None,2),name=\"labels\")\n",
    "    \n",
    "logits = vggnet(X)\n",
    "with tf.name_scope(\"loss\"):    \n",
    "    train_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y), name=\"loss\")\n",
    "    tf.summary.scalar(\"loss\", train_loss)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(train_loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(train_loss)\n",
    "print(\"optimizer set\")\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "print(\"accuracy set\")\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "# init = \n",
    "# print(\"initializing the variables\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"session created.\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(\"writing tensorboard\")\n",
    "    writer = tf.summary.FileWriter(\"/tmp/tensorboard/cat-dog-vgg1\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    print(\"writer added to graph\")\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        print(\"processing epoch \"+str(epoch))\n",
    "        print(\"total number of batches in 1 epoch: \"+ str(len(train_data)/BATCH_SIZE))\n",
    "        \n",
    "        for i in range(0,len(train_data),BATCH_SIZE): \n",
    "#             print(i)\n",
    "            print(\"Batch:\" + str((i/BATCH_SIZE)+1))\n",
    "            \n",
    "            minibatch_X, minibatch_Y = prepare_data(train_data[i:i+BATCH_SIZE])\n",
    "            \n",
    "            if i % (BATCH_SIZE*4) == 0:  # Record summaries and test-set accuracy\n",
    "                summary = sess.run(merged_summary, feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "                writer.add_summary(summary, i + epoch*len(train_data))\n",
    "            \n",
    "            #sess.run(optimizer,feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "            _ , temp_loss, temp_accuracy = sess.run([optimizer,train_loss,accuracy],feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "    \n",
    "            #writer.add_summary(summary, i)\n",
    "            print(\"temp_loss: \"+str(temp_loss))\n",
    "            \n",
    "#             if i % (BATCH_SIZE*8) == 0 and i!=0:\n",
    "#                 print(\"validating...\")\n",
    "#                 for v in range(0,len(val_data),50):\n",
    "#                     val_x,val_y = prepare_data(val_data[v:v+BATCH_SIZE])\n",
    "#                     summary = sess.run(merged_summary, feed_dict={X:val_x, Y:val_y})\n",
    "#                     writer.add_summary(summary, v + epoch*len(val_data))\n",
    "            \n",
    "            \n",
    "#             print(\"temp_accuracy: \"+str(temp_accuracy))\n",
    "#     train_accuracy = accuracy.eval({X: train_data, Y: train_labels})\n",
    "#     val_data, val_labels = prepare_data(val_data)\n",
    "#     val_accuracy = accuracy.eval({X: val_data, Y: val_labels})\n",
    "# #     print(\"Train Accuracy:\", train_accuracy)\n",
    "#     print(\"Validation Accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-8a7bc990c777>:15: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8a7bc990c777>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mminibatch_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mnum_minibatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# number of minibatches of size minibatch_size in the train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mminibatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_mini_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "learning_rate = 0.009\n",
    "input_channels = 3\n",
    "num_epochs= 20\n",
    "minibatch_size = 64\n",
    "with tf.name_scope('inputs'):\n",
    "    X = tf.placeholder(tf.float32, shape=(None,227, 227, 3),name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32,shape=(None,1000),name=\"labels\")\n",
    "\n",
    "logits = alexnet_model(X,input_channels)\n",
    "\n",
    "with tf.name_scope(\"loss\"):    \n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=Y), name=\"loss\")\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "summ = tf.summary.merge_all()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    writer = tf.summary.FileWriter(\"/tmp/test-tensorboard/1\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    for epoch in range(num_epochs):\n",
    "        minibatch_cost = 0.\n",
    "        num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "            # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "            ### START CODE HERE ### (1 line)\n",
    "            _ , temp_loss, temp_accuracy = sess.run([optimizer,loss,accuracy],feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            minibatch_cost += temp_loss / num_minibatches\n",
    "\n",
    "        \n",
    "        # Print the cost every epoch\n",
    "        if print_cost == True and epoch % 5 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "        if print_cost == True and epoch % 1 == 0:\n",
    "            costs.append(minibatch_cost)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
