{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "# import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import json\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "# %matplotlib inline\n",
    "# np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 224\n",
    "data_file = \"dataset.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file =\"vgg16_weights.npz\"\n",
    "weights = np.load(weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv1_1_W (3, 3, 3, 64)\n",
      "1 conv1_1_b (64,)\n",
      "2 conv1_2_W (3, 3, 64, 64)\n",
      "3 conv1_2_b (64,)\n",
      "4 conv2_1_W (3, 3, 64, 128)\n",
      "5 conv2_1_b (128,)\n",
      "6 conv2_2_W (3, 3, 128, 128)\n",
      "7 conv2_2_b (128,)\n",
      "8 conv3_1_W (3, 3, 128, 256)\n",
      "9 conv3_1_b (256,)\n",
      "10 conv3_2_W (3, 3, 256, 256)\n",
      "11 conv3_2_b (256,)\n",
      "12 conv3_3_W (3, 3, 256, 256)\n",
      "13 conv3_3_b (256,)\n",
      "14 conv4_1_W (3, 3, 256, 512)\n",
      "15 conv4_1_b (512,)\n",
      "16 conv4_2_W (3, 3, 512, 512)\n",
      "17 conv4_2_b (512,)\n",
      "18 conv4_3_W (3, 3, 512, 512)\n",
      "19 conv4_3_b (512,)\n",
      "20 conv5_1_W (3, 3, 512, 512)\n",
      "21 conv5_1_b (512,)\n",
      "22 conv5_2_W (3, 3, 512, 512)\n",
      "23 conv5_2_b (512,)\n",
      "24 conv5_3_W (3, 3, 512, 512)\n",
      "25 conv5_3_b (512,)\n",
      "26 fc6_W (25088, 4096)\n",
      "27 fc6_b (4096,)\n",
      "28 fc7_W (4096, 4096)\n",
      "29 fc7_b (4096,)\n",
      "30 fc8_W (4096, 1000)\n",
      "31 fc8_b (1000,)\n"
     ]
    }
   ],
   "source": [
    "keys = sorted(weights.keys())\n",
    "for index, key in enumerate(keys):\n",
    "    print(index, key, np.shape(weights[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(input_data,filter_size,stride_size,pad=\"SAME\",name=\"conv\",weight_name=\"W1\",bias_name=\"b1\"):\n",
    "    print(\"creating layer :\"+name)\n",
    "    with tf.name_scope(name):\n",
    "        W = tf.Variable(weights[name+\"_W\"],name=\"weight\")\n",
    "        b = tf.Variable(weights[name+\"_b\"],name=\"bias\")\n",
    "        \n",
    "        conv = tf.nn.conv2d(input_data, W, strides=stride_size, padding=pad)\n",
    "        out = tf.nn.bias_add(conv, b)\n",
    "        act = tf.nn.relu(out)\n",
    "        tf.summary.histogram(\"weights\", W)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vggnet_v1(input_data):\n",
    "    print(\"creating model\")\n",
    "    output=[]\n",
    "    layer_input = input_data\n",
    "    \n",
    "    print(\"reading vgg16 json file\")\n",
    "    vgg = json.load(open(\"vgg16.json\"))\n",
    "   \n",
    "    for index,layer in enumerate(vgg):\n",
    "        if \"conv\" in layer:\n",
    "            output = conv_layer(layer_input,vgg[layer][\"weights\"],vgg[layer][\"stride\"],vgg[layer][\"pad\"],layer,\"weight\"+str(index),\"bias\"+str(index))\n",
    "        elif \"pool\" in layer:\n",
    "            output = tf.nn.max_pool(layer_input, ksize=vgg[layer][\"ksize\"], strides=vgg[layer][\"strides\"], padding=vgg[layer][\"padding\"],name=layer)\n",
    "        layer_input = output\n",
    "    \n",
    "    pool5_flat = tf.contrib.layers.flatten(output)\n",
    "\n",
    "    print(\"creating layer : fc6\")\n",
    "    with tf.name_scope('fc6'):\n",
    "        fc6_W = tf.Variable(weights[\"fc6_W\"],name=\"fc6_W\")\n",
    "        fc6_b= tf.Variable(weights[\"fc6_b\"],name=\"fc6_b\")\n",
    "\n",
    "        fc6 = tf.nn.bias_add(tf.matmul(pool5_flat, fc6_W), fc6_b)\n",
    "        fc6 = tf.nn.relu(fc6)\n",
    "    \n",
    "#     fc6 = tf.contrib.layers.fully_connected(P3,4096, weights_initializer=fc6_W, biases_initializer=fc6_b, scope=\"fc6\")\n",
    "#     tf.summary.histogram(\"fc6/relu\", fc6)\n",
    "    \n",
    "    dropout1 = tf.layers.dropout(inputs=fc6, rate=0.5)\n",
    "\n",
    "    print(\"creating layer : fc7\")\n",
    "    with tf.name_scope('fc7'):\n",
    "        fc7_W = tf.Variable(weights[\"fc7_W\"],name=\"fc7_W\")\n",
    "        fc7_b= tf.Variable(weights[\"fc7_b\"],name=\"fc7_b\")\n",
    "\n",
    "        fc7 = tf.nn.bias_add(tf.matmul(dropout1, fc7_W), fc7_b)\n",
    "        fc7 = tf.nn.relu(fc7)\n",
    "    \n",
    "#     fc7 = tf.contrib.layers.fully_connected(dropout1,4096, weights_initializer=fc7_W, biases_initializer=fc7_b, scope=\"fc7\")\n",
    "#     tf.summary.histogram(\"fc7/relu\", fc7)\n",
    "    \n",
    "    dropout2 = tf.layers.dropout(inputs=fc7, rate=0.5)\n",
    "    \n",
    "    print(\"creating layer : fc8\")\n",
    "    fc8 = tf.contrib.layers.fully_connected(dropout2,2, biases_initializer=tf.constant_initializer(1.0), activation_fn=None, scope=\"fc8\")\n",
    "    tf.summary.histogram(\"fc8\", fc8)\n",
    "    \n",
    "    \n",
    "    print(\"model completed.\")\n",
    "    ##NEED FULLY CONNECTED LAYER\n",
    "    return fc8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    newData=[]\n",
    "    labels=[]\n",
    "    print(\"preparing data....\")\n",
    "    for sample in data:\n",
    "        img_path,label = sample.strip().split(\" \")\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((IMG_SIZE,IMG_SIZE))\n",
    "        img = np.array(img)\n",
    "        img = img/255\n",
    "        newData.append(img)\n",
    "        if \"cat\" in img_path:\n",
    "            labels.append(np.array([1,0]))\n",
    "        elif \"dog\" in img_path:\n",
    "            labels.append(np.array([0,1]))\n",
    "    print(\"preparing data completed.\")\n",
    "    return np.array(newData),np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "f = open(data_file,\"r\")\n",
    "data = f.readlines()\n",
    "random.shuffle(data)\n",
    "print(len(data))\n",
    "train_data = data[:20000]\n",
    "val_data = data[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n",
      "reading vgg16 json file\n",
      "creating layer :conv1_1\n",
      "creating layer :conv1_2\n",
      "creating layer :conv2_1\n",
      "creating layer :conv2_2\n",
      "creating layer :conv3_1\n",
      "creating layer :conv3_2\n",
      "creating layer :conv3_3\n",
      "creating layer :conv4_1\n",
      "creating layer :conv4_2\n",
      "creating layer :conv4_3\n",
      "creating layer :conv5_1\n",
      "creating layer :conv5_2\n",
      "creating layer :conv5_3\n",
      "creating layer : fc6\n",
      "creating layer : fc7\n",
      "creating layer : fc8\n",
      "model completed.\n",
      "WARNING:tensorflow:From <ipython-input-10-7df4ae1633c8>:11: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "optimizer set\n",
      "accuracy set\n",
      "session created.\n",
      "initializing global variables\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "learning_rate = 0.001\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    X = tf.placeholder(tf.float32, shape=(None,IMG_SIZE,IMG_SIZE , 3),name=\"X\")\n",
    "    tf.summary.image('input', X, BATCH_SIZE)\n",
    "    Y = tf.placeholder(tf.float32,shape=(None,2),name=\"labels\")\n",
    "    \n",
    "logits = vggnet_v1(X)\n",
    "with tf.name_scope(\"loss\"):    \n",
    "    train_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y), name=\"loss\")\n",
    "    tf.summary.scalar(\"loss\", train_loss)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(train_loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(train_loss)\n",
    "print(\"optimizer set\")\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "print(\"accuracy set\")\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "# init = \n",
    "# print(\"initializing the variables\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"session created.\")\n",
    "    print(\"initializing global variables\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(\"writing tensorboard\")\n",
    "    writer = tf.summary.FileWriter(\"/tmp/tensorboard/cat-dog-vgg0\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    print(\"writer added to graph\")\n",
    "    \n",
    "#     for epoch in range(EPOCHS):\n",
    "        \n",
    "#         print(\"processing epoch \"+str(epoch))\n",
    "#         print(\"total number of batches in 1 epoch: \"+ str(len(train_data)/BATCH_SIZE))\n",
    "        \n",
    "#         for i in range(0,len(train_data),BATCH_SIZE): \n",
    "# #             print(i)\n",
    "#             print(\"Batch:\" + str((i/BATCH_SIZE)+1))\n",
    "            \n",
    "#             minibatch_X, minibatch_Y = prepare_data(train_data[i:i+BATCH_SIZE])\n",
    "#             #sess.run(optimizer,feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "#             _ , temp_loss, temp_accuracy = sess.run([optimizer,train_loss,accuracy],feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "            \n",
    "#             if i % (BATCH_SIZE*15) == 0 and i >200:  # Record summaries and test-set accuracy\n",
    "#                 summary = sess.run(merged_summary, feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "#                 writer.add_summary(summary, i + epoch*len(train_data))\n",
    "            \n",
    "#             #writer.add_summary(summary, i)\n",
    "#             print(\"temp_loss: \"+str(temp_loss))\n",
    "            \n",
    "#             if i % (BATCH_SIZE*50) == 0 and i>=1000:\n",
    "#                 print(\"validating...\")\n",
    "#                 val_loss=0\n",
    "#                 val_accuracy = 0\n",
    "#                 val_batches = len(val_data)/50\n",
    "#                 for v in range(0,len(val_data),50):\n",
    "#                     val_x,val_y = prepare_data(val_data[v:v+BATCH_SIZE])\n",
    "#                     temp_loss, temp_accuracy = sess.run([train_loss,accuracy],feed_dict={X:val_x,Y:val_y})\n",
    "#                     val_accuracy+=temp_accuracy\n",
    "#                     val_loss+=temp_loss\n",
    "#                 print(\"validation loss:\")\n",
    "#                 print(val_loss/val_batches)\n",
    "#                 print(\"validation accuracy: \")\n",
    "#                 print(val_accuracy/val_batches)\n",
    "#             print(\"temp_accuracy: \"+str(temp_accuracy))\n",
    "#     train_accuracy = accuracy.eval({X: train_data, Y: train_labels})\n",
    "#     val_data, val_labels = prepare_data(val_data)\n",
    "#     val_accuracy = accuracy.eval({X: val_data, Y: val_labels})\n",
    "# #     print(\"Train Accuracy:\", train_accuracy)\n",
    "#     print(\"Validation Accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
