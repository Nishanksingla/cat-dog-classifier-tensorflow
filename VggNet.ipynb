{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "# import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import json\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "# %matplotlib inline\n",
    "# np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 50\n",
    "IMG_SIZE = 224\n",
    "data_file = \"dataset.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(input_data,filter_size,stride_size,pad=\"SAME\",name=\"conv\",weight_name=\"W1\",bias_name=\"b1\"):\n",
    "    print(\"creating layer :\"+name)\n",
    "    with tf.name_scope(name):\n",
    "        W = tf.get_variable(weight_name, shape = filter_size, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        biases = tf.get_variable(bias_name, shape = [filter_size[-1]], initializer = tf.constant_initializer(0.0))\n",
    "        \n",
    "        conv = tf.nn.conv2d(input_data, W, strides=stride_size, padding=pad)\n",
    "        out = tf.nn.bias_add(conv, biases)\n",
    "        act = tf.nn.relu(out)\n",
    "        tf.summary.histogram(\"biases\", biases)\n",
    "        tf.summary.histogram(\"weights\", W)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vggnet_v1(input_data):\n",
    "    print(\"creating model\")\n",
    "    output=[]\n",
    "    layer_input = input_data\n",
    "    \n",
    "    print(\"reading vgg16 json file\")\n",
    "    vgg = json.load(open(\"vgg16.json\"),object_pairs_hook=OrderedDict)\n",
    "   \n",
    "    for index,layer in enumerate(vgg):\n",
    "        if \"conv\" in layer:\n",
    "            output = conv_layer(layer_input,vgg[layer][\"weights\"],vgg[layer][\"stride\"],vgg[layer][\"pad\"],layer,\"weight\"+str(index),\"bias\"+str(index))\n",
    "        elif \"pool\" in layer:\n",
    "            output = tf.nn.max_pool(layer_input, ksize=vgg[layer][\"ksize\"], strides=vgg[layer][\"strides\"], padding=vgg[layer][\"padding\"],name=layer)\n",
    "        layer_input = output\n",
    "    \n",
    "    P3 = tf.contrib.layers.flatten(output)\n",
    "    \n",
    "    fc6 = tf.contrib.layers.fully_connected(P3,4096,biases_initializer=tf.constant_initializer(1.0), scope=\"fc6\")\n",
    "    tf.summary.histogram(\"fc6/relu\", fc6)\n",
    "    \n",
    "    dropout1 = tf.layers.dropout(inputs=fc6, rate=0.5)\n",
    "    \n",
    "    fc7 = tf.contrib.layers.fully_connected(dropout1,4096,biases_initializer=tf.constant_initializer(1.0), scope=\"fc7\")\n",
    "    tf.summary.histogram(\"fc7/relu\", fc7)\n",
    "    \n",
    "    dropout2 = tf.layers.dropout(inputs=fc7, rate=0.5)\n",
    "    \n",
    "    fc8 = tf.contrib.layers.fully_connected(dropout2,2, biases_initializer=tf.constant_initializer(1.0), activation_fn=None, scope=\"fc8\")\n",
    "    tf.summary.histogram(\"fc8\", fc8)\n",
    "    \n",
    "    \n",
    "    print(\"model completed.\")\n",
    "    ##NEED FULLY CONNECTED LAYER\n",
    "    return fc8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vggnet_v2(input_data):\n",
    "    \n",
    "    # conv1_1\n",
    "    conv1_1 = conv_layer(input_data,[3,3,3,64],[1, 1, 1, 1],\"SAME\",\"conv1_1\",\"weight1\")\n",
    "\n",
    "    # conv1_2\n",
    "    conv1_2 = conv_layer(conv1_1,[3,3,64,64],[1, 1, 1, 1],\"SAME\",\"conv1_2\",\"weight2\")\n",
    "        \n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(conv1_2,ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],padding=\"SAME\",name='pool1')\n",
    "        \n",
    "    # conv2_1\n",
    "    conv2_1 = conv_layer(pool1,[3, 3, 64, 128],[1, 1, 1, 1],\"SAME\",\"conv2_1\",\"weight3\")\n",
    "        \n",
    "    # conv2_2\n",
    "    conv2_2 = conv_layer(conv2_1,[3,3,128,128],[1, 1, 1, 1],\"SAME\",\"conv2_2\",\"weight4\")\n",
    "\n",
    "    # pool2\n",
    "    pool2 = tf.nn.max_pool(conv2_2,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool2')\n",
    "\n",
    "    # conv3_1\n",
    "    conv3_1 = conv_layer(pool2,[3,3,128,256],[1, 1, 1, 1],\"SAME\",\"conv3_1\",\"weight5\")\n",
    "    \n",
    "    # conv3_2\n",
    "    conv3_2 = conv_layer(conv3_1,[3,3,256,256],[1, 1, 1, 1],\"SAME\",\"conv3_2\",\"weight6\")\n",
    "    \n",
    "    # conv3_3\n",
    "    conv3_3 = conv_layer(conv3_2,[3,3,256,256],[1, 1, 1, 1],\"SAME\",\"conv3_3\",\"weight7\")\n",
    "\n",
    "    # pool3\n",
    "    pool3 = tf.nn.max_pool(conv3_3,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool3')\n",
    "\n",
    "    # conv4_1\n",
    "    conv4_1 = conv_layer(pool3,[3, 3, 256, 512],[1, 1, 1, 1],\"SAME\",\"conv4_1\",\"weight8\")\n",
    "        \n",
    "    # conv4_2\n",
    "    conv4_2 = conv_layer(conv4_1,[3, 3, 512, 512],[1, 1, 1, 1],\"SAME\",\"conv4_2\",\"weight9\")\n",
    "\n",
    "    # conv4_3\n",
    "    conv4_3 = conv_layer(conv4_2,[3, 3, 512, 512],[1, 1, 1, 1],\"SAME\",\"conv4_3\",\"weight10\")\n",
    "\n",
    "    # pool4\n",
    "    pool4 = tf.nn.max_pool(conv4_3,ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool4')\n",
    "\n",
    "    # conv5_1\n",
    "    conv5_1 = conv_layer(pool4,[3, 3, 512, 512],[1, 1, 1, 1],\"SAME\",\"conv5_1\",\"weight11\")\n",
    "   \n",
    "    # conv5_2\n",
    "    conv5_2 = conv_layer(conv5_1,[3, 3, 512, 512],[1, 1, 1, 1],\"SAME\",\"conv5_2\",\"weight12\")\n",
    "\n",
    "    # conv5_3\n",
    "    conv5_3 = conv_layer(conv5_2,[3, 3, 512, 512],[1, 1, 1, 1],\"SAME\",\"conv5_3\",\"weight13\")\n",
    "\n",
    "    # pool5\n",
    "    pool5 = tf.nn.max_pool(conv5_3, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1],padding='SAME',name='pool5')\n",
    "    \n",
    "    flatten = tf.contrib.layers.flatten(pool5)\n",
    "    print(\"flatten\")\n",
    "    print(flatten)\n",
    "    \n",
    "    fc6 = tf.contrib.layers.fully_connected(flatten,4096,scope=\"fc6\")\n",
    "    tf.summary.histogram(\"fc6/relu\", fc6)\n",
    "    \n",
    "#     dropout1 = tf.layers.dropout(inputs=fc6, rate=0.5)\n",
    "    \n",
    "    fc7 = tf.contrib.layers.fully_connected(fc6,4096,scope=\"fc7\")\n",
    "    tf.summary.histogram(\"fc7/relu\", fc7)\n",
    "    \n",
    "#     dropout = tf.layers.dropout(inputs=fc7, rate=0.5)\n",
    "    \n",
    "    fc8 = tf.contrib.layers.fully_connected(fc7,2,activation_fn=None,scope=\"fc8\")\n",
    "    tf.summary.histogram(\"fc8\", fc8)\n",
    "    print(\"fc8:\")\n",
    "    print(fc8)\n",
    "    print(\"model completed.\")\n",
    "    return fc8\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vggnet_v3(images):\n",
    "    \n",
    "    # conv1_1\n",
    "    with tf.name_scope('conv1_1') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name='weights')\n",
    "        conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                             trainable=True, name='biases')\n",
    "        out = tf.nn.bias_add(conv, biases)\n",
    "        conv1_1 = tf.nn.relu(out, name=scope)\n",
    "        \n",
    "\n",
    "    # conv1_2\n",
    "    with tf.name_scope('conv1_2') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name='weights')\n",
    "        conv = tf.nn.conv2d(conv1_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n",
    "                             trainable=True, name='biases')\n",
    "        out = tf.nn.bias_add(conv, biases)\n",
    "        conv1_2 = tf.nn.relu(out, name=scope)\n",
    "        \n",
    "\n",
    "    # pool1\n",
    "    pool1 = tf.nn.max_pool(conv1_2,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding='SAME',\n",
    "                           name='pool1')\n",
    "\n",
    "    # conv2_1\n",
    "    with tf.name_scope('conv2_1') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name='weights')\n",
    "        conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                             trainable=True, name='biases')\n",
    "        out = tf.nn.bias_add(conv, biases)\n",
    "        conv2_1 = tf.nn.relu(out, name=scope)\n",
    "        \n",
    "\n",
    "    # conv2_2\n",
    "    with tf.name_scope('conv2_2') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name='weights')\n",
    "        conv = tf.nn.conv2d(conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                             trainable=True, name='biases')\n",
    "        out = tf.nn.bias_add(conv, biases)\n",
    "        conv2_2 = tf.nn.relu(out, name=scope)\n",
    "        \n",
    "\n",
    "    # pool2\n",
    "    pool2 = tf.nn.max_pool(conv2_2,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding='SAME',\n",
    "                           name='pool2')\n",
    "\n",
    "    # conv3_1\n",
    "    with tf.name_scope('conv3_1') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name='weights')\n",
    "        conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                             trainable=True, name='biases')\n",
    "        out = tf.nn.bias_add(conv, biases)\n",
    "        conv3_1 = tf.nn.relu(out, name=scope)\n",
    "        \n",
    "\n",
    "    # conv3_2\n",
    "    with tf.name_scope('conv3_2') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name='weights')\n",
    "        conv = tf.nn.conv2d(conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                             trainable=True, name='biases')\n",
    "        out = tf.nn.bias_add(conv, biases)\n",
    "        conv3_2 = tf.nn.relu(out, name=scope)\n",
    "        \n",
    "\n",
    "    # conv3_3\n",
    "    with tf.name_scope('conv3_3') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name='weights')\n",
    "        conv = tf.nn.conv2d(conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n",
    "                             trainable=True, name='biases')\n",
    "        out = tf.nn.bias_add(conv, biases)\n",
    "        conv3_3 = tf.nn.relu(out, name=scope)\n",
    "        \n",
    "\n",
    "    # pool3\n",
    "    pool3 = tf.nn.max_pool(conv3_3,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding='SAME',\n",
    "                           name='pool3')\n",
    "\n",
    "    # conv4_1\n",
    "    with tf.name_scope('conv4_1') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name='weights')\n",
    "        conv = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                             trainable=True, name='biases')\n",
    "        out = tf.nn.bias_add(conv, biases)\n",
    "        conv4_1 = tf.nn.relu(out, name=scope)\n",
    "        \n",
    "\n",
    "    # conv4_2\n",
    "    with tf.name_scope('conv4_2') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name='weights')\n",
    "        conv = tf.nn.conv2d(conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                             trainable=True, name='biases')\n",
    "        out = tf.nn.bias_add(conv, biases)\n",
    "        conv4_2 = tf.nn.relu(out, name=scope)\n",
    "        \n",
    "\n",
    "    # conv4_3\n",
    "    with tf.name_scope('conv4_3') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name='weights')\n",
    "        conv = tf.nn.conv2d(conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                             trainable=True, name='biases')\n",
    "        out = tf.nn.bias_add(conv, biases)\n",
    "        conv4_3 = tf.nn.relu(out, name=scope)\n",
    "        \n",
    "\n",
    "    # pool4\n",
    "    pool4 = tf.nn.max_pool(conv4_3,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding='SAME',\n",
    "                           name='pool4')\n",
    "\n",
    "    # conv5_1\n",
    "    with tf.name_scope('conv5_1') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name='weights')\n",
    "        conv = tf.nn.conv2d(pool4, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                             trainable=True, name='biases')\n",
    "        out = tf.nn.bias_add(conv, biases)\n",
    "        conv5_1 = tf.nn.relu(out, name=scope)\n",
    "        \n",
    "\n",
    "    # conv5_2\n",
    "    with tf.name_scope('conv5_2') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name='weights')\n",
    "        conv = tf.nn.conv2d(conv5_1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                             trainable=True, name='biases')\n",
    "        out = tf.nn.bias_add(conv, biases)\n",
    "        conv5_2 = tf.nn.relu(out, name=scope)\n",
    "        \n",
    "\n",
    "    # conv5_3\n",
    "    with tf.name_scope('conv5_3') as scope:\n",
    "        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n",
    "                                                 stddev=1e-1), name='weights')\n",
    "        conv = tf.nn.conv2d(conv5_2, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n",
    "                             trainable=True, name='biases')\n",
    "        out = tf.nn.bias_add(conv, biases)\n",
    "        conv5_3 = tf.nn.relu(out, name=scope)\n",
    "        \n",
    "\n",
    "    # pool5\n",
    "    pool5 = tf.nn.max_pool(conv5_3,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding='SAME',\n",
    "                           name='pool4')\n",
    "    \n",
    "    \n",
    "    # fc1\n",
    "    with tf.name_scope('fc1') as scope:\n",
    "        shape = int(np.prod(pool5.get_shape()[1:]))\n",
    "        fc1w = tf.Variable(tf.truncated_normal([shape, 4096],\n",
    "                                                     dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "        fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "                             trainable=True, name='biases')\n",
    "        pool5_flat = tf.reshape(pool5, [-1, shape])\n",
    "        fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)\n",
    "        fc1 = tf.nn.relu(fc1l)\n",
    "\n",
    "\n",
    "    # fc2\n",
    "    with tf.name_scope('fc2') as scope:\n",
    "        fc2w = tf.Variable(tf.truncated_normal([4096, 4096],\n",
    "                                                     dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "        fc2b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n",
    "                             trainable=True, name='biases')\n",
    "        fc2l = tf.nn.bias_add(tf.matmul(fc1, fc2w), fc2b)\n",
    "        fc2 = tf.nn.relu(fc2l)\n",
    "\n",
    "\n",
    "    # fc3\n",
    "    with tf.name_scope('fc3') as scope:\n",
    "        fc3w = tf.Variable(tf.truncated_normal([4096, 2],\n",
    "                                                     dtype=tf.float32,\n",
    "                                                     stddev=1e-1), name='weights')\n",
    "        fc3b = tf.Variable(tf.constant(1.0, shape=[2], dtype=tf.float32),\n",
    "                             trainable=True, name='biases')\n",
    "        fc3l = tf.nn.bias_add(tf.matmul(fc2, fc3w), fc3b)\n",
    "#         fc3 = tf.nn.relu(fc3l)\n",
    "\n",
    "    return fc3l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    newData=[]\n",
    "    labels=[]\n",
    "    print(\"preparing data....\")\n",
    "    for sample in data:\n",
    "        img_path,label = sample.strip().split(\" \")\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((IMG_SIZE,IMG_SIZE))\n",
    "        img = np.array(img)\n",
    "        img = img/255\n",
    "        newData.append(img)\n",
    "        if \"cat\" in img_path:\n",
    "            labels.append(np.array([1,0]))\n",
    "        elif \"dog\" in img_path:\n",
    "            labels.append(np.array([0,1]))\n",
    "    print(\"preparing data completed.\")\n",
    "    return np.array(newData),np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "preparing data....\n",
      "preparing data completed.\n",
      "preparing data....\n",
      "preparing data completed.\n"
     ]
    }
   ],
   "source": [
    "f = open(data_file,\"r\")\n",
    "data = f.readlines()\n",
    "random.shuffle(data)\n",
    "print(len(data))\n",
    "train_data = data[:20000]\n",
    "train_X, train_Y = prepare_data(train_data)\n",
    "val_data = data[20000:]\n",
    "val_X, val_Y = prepare_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n",
      "reading vgg16 json file\n",
      "creating layer :conv1_1\n",
      "creating layer :conv1_2\n",
      "creating layer :conv2_1\n",
      "creating layer :conv2_2\n",
      "creating layer :conv3_1\n",
      "creating layer :conv3_2\n",
      "creating layer :conv3_3\n",
      "creating layer :conv4_1\n",
      "creating layer :conv4_2\n",
      "creating layer :conv4_3\n",
      "creating layer :conv5_1\n",
      "creating layer :conv5_2\n",
      "creating layer :conv5_3\n",
      "model completed.\n",
      "optimizer set\n",
      "accuracy set\n",
      "session created.\n",
      "writing tensorboard\n",
      "writer added to graph\n",
      "processing epoch 0\n",
      "total number of batches in 1 epoch: 400.0\n",
      "Batch:1.0\n",
      "temp_loss: 1.01737\n",
      "Batch:2.0\n",
      "temp_loss: 384.513\n",
      "Batch:3.0\n",
      "temp_loss: 87.2746\n",
      "Batch:4.0\n",
      "temp_loss: 71.1724\n",
      "Batch:5.0\n",
      "temp_loss: 33.8528\n",
      "Batch:6.0\n",
      "temp_loss: 5.47547\n",
      "Batch:7.0\n",
      "temp_loss: 0.721761\n",
      "Batch:8.0\n",
      "temp_loss: 1.7492\n",
      "Batch:9.0\n",
      "temp_loss: 10.3293\n",
      "Batch:10.0\n",
      "temp_loss: 3.96727\n",
      "Batch:11.0\n",
      "temp_loss: 0.852472\n",
      "Batch:12.0\n",
      "temp_loss: 4.00442\n",
      "Batch:13.0\n",
      "temp_loss: 2.38266\n",
      "Batch:14.0\n",
      "temp_loss: 2.71777\n",
      "Batch:15.0\n",
      "temp_loss: 2.71458\n",
      "Batch:16.0\n",
      "temp_loss: 2.14912\n",
      "Batch:17.0\n",
      "temp_loss: 1.8527\n",
      "Batch:18.0\n",
      "temp_loss: 0.957609\n",
      "Batch:19.0\n",
      "temp_loss: 0.666313\n",
      "Batch:20.0\n",
      "temp_loss: 1.28736\n",
      "Batch:21.0\n",
      "temp_loss: 1.24683\n",
      "Batch:22.0\n",
      "temp_loss: 1.7503\n",
      "Batch:23.0\n",
      "temp_loss: 1.5793\n",
      "Batch:24.0\n",
      "temp_loss: 1.32206\n",
      "Batch:25.0\n",
      "temp_loss: 0.698919\n",
      "Batch:26.0\n",
      "temp_loss: 1.57928\n",
      "Batch:27.0\n",
      "temp_loss: 0.888407\n",
      "Batch:28.0\n",
      "temp_loss: 0.687715\n",
      "Batch:29.0\n",
      "temp_loss: 0.713389\n",
      "Batch:30.0\n",
      "temp_loss: 0.858613\n",
      "Batch:31.0\n",
      "temp_loss: 0.709093\n",
      "Batch:32.0\n",
      "temp_loss: 0.691114\n",
      "Batch:33.0\n",
      "temp_loss: 0.793696\n",
      "Batch:34.0\n",
      "temp_loss: 0.708537\n",
      "Batch:35.0\n",
      "temp_loss: 0.734092\n",
      "Batch:36.0\n",
      "temp_loss: 0.704467\n",
      "Batch:37.0\n",
      "temp_loss: 0.689572\n",
      "Batch:38.0\n",
      "temp_loss: 0.645846\n",
      "Batch:39.0\n",
      "temp_loss: 0.879945\n",
      "Batch:40.0\n",
      "temp_loss: 0.784397\n",
      "Batch:41.0\n",
      "temp_loss: 0.743284\n",
      "Batch:42.0\n",
      "temp_loss: 0.695056\n",
      "Batch:43.0\n",
      "temp_loss: 0.707821\n",
      "Batch:44.0\n",
      "temp_loss: 0.642224\n",
      "Batch:45.0\n",
      "temp_loss: 0.821946\n",
      "Batch:46.0\n",
      "temp_loss: 0.837304\n",
      "Batch:47.0\n",
      "temp_loss: 0.712077\n",
      "Batch:48.0\n",
      "temp_loss: 0.687595\n",
      "Batch:49.0\n",
      "temp_loss: 0.641052\n",
      "Batch:50.0\n",
      "temp_loss: 1.03678\n",
      "Batch:51.0\n",
      "temp_loss: 0.878955\n",
      "validating...\n",
      "validation loss:\n",
      "0.728375833631\n",
      "validation accuracy: \n",
      "0.505999985337\n",
      "Batch:52.0\n",
      "temp_loss: 0.653449\n",
      "Batch:53.0\n",
      "temp_loss: 0.700427\n",
      "Batch:54.0\n",
      "temp_loss: 0.714357\n",
      "Batch:55.0\n",
      "temp_loss: 0.751293\n",
      "Batch:56.0\n",
      "temp_loss: 0.766622\n",
      "Batch:57.0\n",
      "temp_loss: 0.779632\n",
      "Batch:58.0\n",
      "temp_loss: 0.691518\n",
      "Batch:59.0\n",
      "temp_loss: 0.677403\n",
      "Batch:60.0\n",
      "temp_loss: 0.802264\n",
      "Batch:61.0\n",
      "temp_loss: 0.760195\n",
      "Batch:62.0\n",
      "temp_loss: 0.731053\n",
      "Batch:63.0\n",
      "temp_loss: 0.711967\n",
      "Batch:64.0\n",
      "temp_loss: 0.685947\n",
      "Batch:65.0\n",
      "temp_loss: 0.7127\n",
      "Batch:66.0\n",
      "temp_loss: 0.878528\n",
      "Batch:67.0\n",
      "temp_loss: 0.845207\n",
      "Batch:68.0\n",
      "temp_loss: 0.692442\n",
      "Batch:69.0\n",
      "temp_loss: 0.758024\n",
      "Batch:70.0\n",
      "temp_loss: 0.784113\n",
      "Batch:71.0\n",
      "temp_loss: 0.797686\n",
      "Batch:72.0\n",
      "temp_loss: 0.844923\n",
      "Batch:73.0\n",
      "temp_loss: 0.690418\n",
      "Batch:74.0\n",
      "temp_loss: 0.690603\n",
      "Batch:75.0\n",
      "temp_loss: 0.945546\n",
      "Batch:76.0\n",
      "temp_loss: 0.95321\n",
      "Batch:77.0\n",
      "temp_loss: 0.791065\n",
      "Batch:78.0\n",
      "temp_loss: 0.68939\n",
      "Batch:79.0\n",
      "temp_loss: 0.897605\n",
      "Batch:80.0\n",
      "temp_loss: 0.864712\n",
      "Batch:81.0\n",
      "temp_loss: 0.750598\n",
      "Batch:82.0\n",
      "temp_loss: 0.714426\n",
      "Batch:83.0\n",
      "temp_loss: 0.698762\n",
      "Batch:84.0\n",
      "temp_loss: 0.707978\n",
      "Batch:85.0\n",
      "temp_loss: 0.768026\n",
      "Batch:86.0\n",
      "temp_loss: 0.722812\n",
      "Batch:87.0\n",
      "temp_loss: 0.725711\n",
      "Batch:88.0\n",
      "temp_loss: 0.713394\n",
      "Batch:89.0\n",
      "temp_loss: 0.709216\n",
      "Batch:90.0\n",
      "temp_loss: 0.68061\n",
      "Batch:91.0\n",
      "temp_loss: 0.697268\n",
      "Batch:92.0\n",
      "temp_loss: 0.70604\n",
      "Batch:93.0\n",
      "temp_loss: 0.692635\n",
      "Batch:94.0\n",
      "temp_loss: 0.701723\n",
      "Batch:95.0\n",
      "temp_loss: 0.715822\n",
      "Batch:96.0\n",
      "temp_loss: 0.695683\n",
      "Batch:97.0\n",
      "temp_loss: 0.696595\n",
      "Batch:98.0\n",
      "temp_loss: 0.705315\n",
      "Batch:99.0\n",
      "temp_loss: 0.686014\n",
      "Batch:100.0\n",
      "temp_loss: 0.686791\n",
      "Batch:101.0\n",
      "temp_loss: 0.666758\n",
      "validating...\n",
      "validation loss:\n",
      "0.712395476103\n",
      "validation accuracy: \n",
      "0.505999985337\n",
      "Batch:102.0\n",
      "temp_loss: 0.731658\n",
      "Batch:103.0\n",
      "temp_loss: 0.703917\n",
      "Batch:104.0\n",
      "temp_loss: 0.683062\n",
      "Batch:105.0\n",
      "temp_loss: 0.788213\n",
      "Batch:106.0\n",
      "temp_loss: 0.772123\n",
      "Batch:107.0\n",
      "temp_loss: 0.711833\n",
      "Batch:108.0\n",
      "temp_loss: 0.664977\n",
      "Batch:109.0\n",
      "temp_loss: 0.761811\n",
      "Batch:110.0\n",
      "temp_loss: 0.907576\n",
      "Batch:111.0\n",
      "temp_loss: 0.65085\n",
      "Batch:112.0\n",
      "temp_loss: 0.712797\n",
      "Batch:113.0\n",
      "temp_loss: 0.684671\n",
      "Batch:114.0\n",
      "temp_loss: 0.721194\n",
      "Batch:115.0\n",
      "temp_loss: 0.844598\n",
      "Batch:116.0\n",
      "temp_loss: 0.857553\n",
      "Batch:117.0\n",
      "temp_loss: 0.705137\n",
      "Batch:118.0\n",
      "temp_loss: 0.71963\n",
      "Batch:119.0\n",
      "temp_loss: 0.886189\n",
      "Batch:120.0\n",
      "temp_loss: 0.737996\n",
      "Batch:121.0\n",
      "temp_loss: 0.75139\n",
      "Batch:122.0\n",
      "temp_loss: 0.715061\n",
      "Batch:123.0\n",
      "temp_loss: 0.7208\n",
      "Batch:124.0\n",
      "temp_loss: 0.805353\n",
      "Batch:125.0\n",
      "temp_loss: 0.731959\n",
      "Batch:126.0\n",
      "temp_loss: 0.695616\n",
      "Batch:127.0\n",
      "temp_loss: 0.643577\n",
      "Batch:128.0\n",
      "temp_loss: 0.729188\n",
      "Batch:129.0\n",
      "temp_loss: 0.92421\n",
      "Batch:130.0\n",
      "temp_loss: 0.708844\n",
      "Batch:131.0\n",
      "temp_loss: 0.692607\n",
      "Batch:132.0\n",
      "temp_loss: 0.741728\n",
      "Batch:133.0\n",
      "temp_loss: 0.857783\n",
      "Batch:134.0\n",
      "temp_loss: 0.688868\n",
      "Batch:135.0\n",
      "temp_loss: 0.689987\n",
      "Batch:136.0\n",
      "temp_loss: 0.706293\n",
      "Batch:137.0\n",
      "temp_loss: 0.726395\n",
      "Batch:138.0\n",
      "temp_loss: 0.708556\n",
      "Batch:139.0\n",
      "temp_loss: 0.679489\n",
      "Batch:140.0\n",
      "temp_loss: 0.6955\n",
      "Batch:141.0\n",
      "temp_loss: 0.694602\n",
      "Batch:142.0\n",
      "temp_loss: 0.692713\n",
      "Batch:143.0\n",
      "temp_loss: 0.691581\n",
      "Batch:144.0\n",
      "temp_loss: 0.69469\n",
      "Batch:145.0\n",
      "temp_loss: 0.686778\n",
      "Batch:146.0\n",
      "temp_loss: 0.741217\n",
      "Batch:147.0\n",
      "temp_loss: 0.687236\n",
      "Batch:148.0\n",
      "temp_loss: 0.6977\n",
      "Batch:149.0\n",
      "temp_loss: 0.697949\n",
      "Batch:150.0\n",
      "temp_loss: 0.690698\n",
      "Batch:151.0\n",
      "temp_loss: 0.685648\n",
      "validating...\n",
      "validation loss:\n",
      "0.702585321665\n",
      "validation accuracy: \n",
      "0.49399998188\n",
      "Batch:152.0\n",
      "temp_loss: 0.711158\n",
      "Batch:153.0\n",
      "temp_loss: 0.674173\n",
      "Batch:154.0\n",
      "temp_loss: 0.714977\n",
      "Batch:155.0\n",
      "temp_loss: 0.713206\n",
      "Batch:156.0\n",
      "temp_loss: 0.695842\n",
      "Batch:157.0\n",
      "temp_loss: 0.706802\n",
      "Batch:158.0\n",
      "temp_loss: 0.736547\n",
      "Batch:159.0\n",
      "temp_loss: 0.727459\n",
      "Batch:160.0\n",
      "temp_loss: 0.677823\n",
      "Batch:161.0\n",
      "temp_loss: 0.80372\n",
      "Batch:162.0\n",
      "temp_loss: 0.845195\n",
      "Batch:163.0\n",
      "temp_loss: 0.685988\n",
      "Batch:164.0\n",
      "temp_loss: 0.664641\n",
      "Batch:165.0\n",
      "temp_loss: 0.84701\n",
      "Batch:166.0\n",
      "temp_loss: 0.788197\n",
      "Batch:167.0\n",
      "temp_loss: 0.747076\n",
      "Batch:168.0\n",
      "temp_loss: 0.747519\n",
      "Batch:169.0\n",
      "temp_loss: 0.76332\n",
      "Batch:170.0\n",
      "temp_loss: 0.761735\n",
      "Batch:171.0\n",
      "temp_loss: 0.732298\n",
      "Batch:172.0\n",
      "temp_loss: 0.656504\n",
      "Batch:173.0\n",
      "temp_loss: 0.981485\n",
      "Batch:174.0\n",
      "temp_loss: 0.845914\n",
      "Batch:175.0\n",
      "temp_loss: 0.71134\n",
      "Batch:176.0\n",
      "temp_loss: 0.696074\n",
      "Batch:177.0\n",
      "temp_loss: 0.681852\n",
      "Batch:178.0\n",
      "temp_loss: 0.958049\n",
      "Batch:179.0\n",
      "temp_loss: 0.887464\n",
      "Batch:180.0\n",
      "temp_loss: 0.769326\n",
      "Batch:181.0\n",
      "temp_loss: 0.728236\n",
      "Batch:182.0\n",
      "temp_loss: 0.759002\n",
      "Batch:183.0\n",
      "temp_loss: 1.21989\n",
      "Batch:184.0\n",
      "temp_loss: 0.735895\n",
      "Batch:185.0\n",
      "temp_loss: 0.77212\n",
      "Batch:186.0\n",
      "temp_loss: 0.744678\n",
      "Batch:187.0\n",
      "temp_loss: 0.871707\n",
      "Batch:188.0\n",
      "temp_loss: 0.843525\n",
      "Batch:189.0\n",
      "temp_loss: 0.670986\n",
      "Batch:190.0\n",
      "temp_loss: 0.709059\n",
      "Batch:191.0\n",
      "temp_loss: 0.80888\n",
      "Batch:192.0\n",
      "temp_loss: 0.782776\n",
      "Batch:193.0\n",
      "temp_loss: 0.693496\n",
      "Batch:194.0\n",
      "temp_loss: 0.807494\n",
      "Batch:195.0\n",
      "temp_loss: 0.863844\n",
      "Batch:196.0\n",
      "temp_loss: 0.735719\n",
      "Batch:197.0\n",
      "temp_loss: 0.739587\n",
      "Batch:198.0\n",
      "temp_loss: 0.81492\n",
      "Batch:199.0\n",
      "temp_loss: 0.778041\n",
      "Batch:200.0\n",
      "temp_loss: 0.706035\n",
      "Batch:201.0\n",
      "temp_loss: 0.690762\n",
      "validating...\n",
      "validation loss:\n",
      "0.745365566015\n",
      "validation accuracy: \n",
      "0.49399998188\n",
      "Batch:202.0\n",
      "temp_loss: 0.829468\n",
      "Batch:203.0\n",
      "temp_loss: 0.69288\n",
      "Batch:204.0\n",
      "temp_loss: 0.680293\n",
      "Batch:205.0\n",
      "temp_loss: 0.76185\n",
      "Batch:206.0\n",
      "temp_loss: 0.743683\n",
      "Batch:207.0\n",
      "temp_loss: 0.731421\n",
      "Batch:208.0\n",
      "temp_loss: 0.686409\n",
      "Batch:209.0\n",
      "temp_loss: 0.785169\n",
      "Batch:210.0\n",
      "temp_loss: 0.79531\n",
      "Batch:211.0\n",
      "temp_loss: 0.730918\n",
      "Batch:212.0\n",
      "temp_loss: 0.718363\n",
      "Batch:213.0\n",
      "temp_loss: 0.75402\n",
      "Batch:214.0\n",
      "temp_loss: 0.727721\n",
      "Batch:215.0\n",
      "temp_loss: 0.695986\n",
      "Batch:216.0\n",
      "temp_loss: 0.647796\n",
      "Batch:217.0\n",
      "temp_loss: 0.809353\n",
      "Batch:218.0\n",
      "temp_loss: 0.844882\n",
      "Batch:219.0\n",
      "temp_loss: 0.757607\n",
      "Batch:220.0\n",
      "temp_loss: 0.771656\n",
      "Batch:221.0\n",
      "temp_loss: 0.976603\n",
      "Batch:222.0\n",
      "temp_loss: 0.846909\n",
      "Batch:223.0\n",
      "temp_loss: 0.695845\n",
      "Batch:224.0\n",
      "temp_loss: 0.710683\n",
      "Batch:225.0\n",
      "temp_loss: 0.863471\n",
      "Batch:226.0\n",
      "temp_loss: 0.713174\n",
      "Batch:227.0\n",
      "temp_loss: 0.681528\n",
      "Batch:228.0\n",
      "temp_loss: 0.644536\n",
      "Batch:229.0\n",
      "temp_loss: 0.815557\n",
      "Batch:230.0\n",
      "temp_loss: 0.841815\n",
      "Batch:231.0\n",
      "temp_loss: 0.653535\n",
      "Batch:232.0\n",
      "temp_loss: 0.693355\n",
      "Batch:233.0\n",
      "temp_loss: 0.751521\n",
      "Batch:234.0\n",
      "temp_loss: 0.713325\n",
      "Batch:235.0\n",
      "temp_loss: 0.721398\n",
      "Batch:236.0\n",
      "temp_loss: 0.699473\n",
      "Batch:237.0\n",
      "temp_loss: 0.680917\n",
      "Batch:238.0\n",
      "temp_loss: 0.711728\n",
      "Batch:239.0\n",
      "temp_loss: 0.665577\n",
      "Batch:240.0\n",
      "temp_loss: 0.724764\n",
      "Batch:241.0\n",
      "temp_loss: 0.69369\n",
      "Batch:242.0\n",
      "temp_loss: 0.684439\n",
      "Batch:243.0\n",
      "temp_loss: 0.738736\n",
      "Batch:244.0\n",
      "temp_loss: 0.721607\n",
      "Batch:245.0\n",
      "temp_loss: 0.712441\n",
      "Batch:246.0\n",
      "temp_loss: 0.690013\n",
      "Batch:247.0\n",
      "temp_loss: 0.675997\n",
      "Batch:248.0\n",
      "temp_loss: 0.851859\n",
      "Batch:249.0\n",
      "temp_loss: 0.70978\n",
      "Batch:250.0\n",
      "temp_loss: 0.697758\n",
      "Batch:251.0\n",
      "temp_loss: 0.685496\n",
      "validating...\n",
      "validation loss:\n",
      "0.7711414814\n",
      "validation accuracy: \n",
      "0.49399998188\n",
      "Batch:252.0\n",
      "temp_loss: 0.781997\n",
      "Batch:253.0\n",
      "temp_loss: 0.735003\n",
      "Batch:254.0\n",
      "temp_loss: 0.692347\n",
      "Batch:255.0\n",
      "temp_loss: 0.702322\n",
      "Batch:256.0\n",
      "temp_loss: 0.733539\n",
      "Batch:257.0\n",
      "temp_loss: 0.8103\n",
      "Batch:258.0\n",
      "temp_loss: 0.692347\n",
      "Batch:259.0\n",
      "temp_loss: 0.739307\n",
      "Batch:260.0\n",
      "temp_loss: 0.728785\n",
      "Batch:261.0\n",
      "temp_loss: 0.687964\n",
      "Batch:262.0\n",
      "temp_loss: 0.71561\n",
      "Batch:263.0\n",
      "temp_loss: 0.685935\n",
      "Batch:264.0\n",
      "temp_loss: 0.84157\n",
      "Batch:265.0\n",
      "temp_loss: 0.701514\n",
      "Batch:387.0\n",
      "temp_loss: 0.809712\n",
      "Batch:388.0\n",
      "temp_loss: 0.732654\n",
      "Batch:389.0\n",
      "temp_loss: 0.757219\n",
      "Batch:390.0\n",
      "temp_loss: 0.680899\n",
      "Batch:370.0\n",
      "temp_loss: 0.68799\n",
      "Batch:371.0\n",
      "temp_loss: 0.694663\n",
      "Batch:372.0\n",
      "temp_loss: 0.703139\n",
      "Batch:373.0\n",
      "temp_loss: 0.696343\n",
      "Batch:374.0\n",
      "temp_loss: 0.695924\n",
      "Batch:375.0\n",
      "temp_loss: 0.700184\n",
      "Batch:376.0\n",
      "temp_loss: 0.701288\n",
      "Batch:377.0\n",
      "temp_loss: 0.693384\n",
      "Batch:378.0\n",
      "temp_loss: 0.694716\n",
      "Batch:379.0\n",
      "temp_loss: 0.693528\n",
      "Batch:380.0\n",
      "temp_loss: 0.692136\n",
      "Batch:381.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "start_learning_rate = 0.01\n",
    "batch = tf.Variable(0, trainable=False)\n",
    "train_size = 20000\n",
    "val_size = 5000\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "  0.001,                # Base learning rate.\n",
    "  batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "  train_size,          # Decay step.\n",
    "  0.1,                # Decay rate.\n",
    "  staircase=True)\n",
    "\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    X = tf.placeholder(tf.float32, shape=(None,IMG_SIZE,IMG_SIZE , 3),name=\"X\")\n",
    "    tf.summary.image('input', X, BATCH_SIZE)\n",
    "    Y = tf.placeholder(tf.float32,shape=(None,2),name=\"labels\")\n",
    "    \n",
    "logits = vggnet_v1(X)\n",
    "with tf.name_scope(\"loss\"):    \n",
    "    train_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y), name=\"loss\")\n",
    "    tf.summary.scalar(\"loss\", train_loss)\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(train_loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(train_loss,global_step=batch)\n",
    "print(\"optimizer set\")\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "print(\"accuracy set\")\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "# init = \n",
    "# print(\"initializing the variables\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"session created.\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(\"writing tensorboard\")\n",
    "    writer = tf.summary.FileWriter(\"/tmp/tensorboard/cat-dog-vgg2\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    print(\"writer added to graph\")\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        print(\"processing epoch \"+str(epoch))\n",
    "        print(\"total number of batches in 1 epoch: \"+ str(len(train_data)/BATCH_SIZE))\n",
    "        \n",
    "        for i in range(0,train_size,BATCH_SIZE): \n",
    "            #print(i)\n",
    "            print(\"Batch:\" + str((i/BATCH_SIZE)+1))\n",
    "            \n",
    "            #minibatch_X, minibatch_Y = prepare_data(train_data[i:i+BATCH_SIZE])\n",
    "            #sess.run(optimizer,feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "            minibatch_X = train_X[i : i + BATCH_SIZE]\n",
    "            minibatch_Y = train_Y[i : i + BATCH_SIZE]\n",
    "            _ , temp_loss, temp_accuracy = sess.run([optimizer,train_loss,accuracy],feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "            \n",
    "            if i % (BATCH_SIZE*10) == 0 and i >200:  # Record summaries and test-set accuracy\n",
    "                summary = sess.run(merged_summary, feed_dict={X:minibatch_X,Y:minibatch_Y})\n",
    "                writer.add_summary(summary, i + epoch*len(train_data))\n",
    "            \n",
    "            #writer.add_summary(summary, i)\n",
    "            print(\"temp_loss: \"+str(temp_loss))\n",
    "            \n",
    "            if i % (BATCH_SIZE*50) == 0 and i>=1000:\n",
    "                print(\"validating...\")\n",
    "                val_loss = 0\n",
    "                val_accuracy = 0\n",
    "                val_batch_size = 50\n",
    "                val_batches = val_size/val_batch_size\n",
    "                for v in range(0,val_size,val_batch_size):\n",
    "                    #val_x,val_y = prepare_data(val_data[v:v+BATCH_SIZE])\n",
    "                    val_minibatch_x = val_X[v : v + val_batch_size]\n",
    "                    val_minibatch_y = val_Y[v : v + val_batch_size]\n",
    "                    temp_loss, temp_accuracy = sess.run([train_loss,accuracy],feed_dict={X:val_minibatch_x,Y:val_minibatch_y})\n",
    "                    val_accuracy+=temp_accuracy\n",
    "                    val_loss+=temp_loss\n",
    "                print(\"validation loss:\")\n",
    "                print(val_loss/val_batches)\n",
    "                print(\"validation accuracy: \")\n",
    "                print(val_accuracy/val_batches)\n",
    "#             print(\"temp_accuracy: \"+str(temp_accuracy))\n",
    "#     train_accuracy = accuracy.eval({X: train_data, Y: train_labels})\n",
    "#     val_data, val_labels = prepare_data(val_data)\n",
    "#     val_accuracy = accuracy.eval({X: val_data, Y: val_labels})\n",
    "# #     print(\"Train Accuracy:\", train_accuracy)\n",
    "#     print(\"Validation Accuracy:\", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
